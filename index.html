<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Digit Recognizer by chapleau</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Digit Recognizer</h1>
        <h2></h2>
        <a href="https://github.com/chapleau/DigitRecog" class="button"><small>View project on</small>GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1>
<a name="digit-recognizer" class="anchor" href="#digit-recognizer"><span class="octicon octicon-link"></span></a>Understanding multiclass classifiers</h1>

<p>This Digit Recognizer package provides an example of the use of supervised (machine) learning methods to classify large datasets. In the present case, the dataset consists in a large set of images representing handwritten digits (0 to 9). The goal is to be able to predict to which class (i.e. which digit) a given instance (image) belongs to. </p>

<p>This dataset has been studied extensively (see <a href="http://yann.lecun.com/exdb/mnist/">Yann LeCun's webpage</a>); current best efforts with state of the art classification methods achieve remarkable success rates. The aim of this digit recognizer is not to improve on those results but to provide a testbed for understanding the output of a classifier and making the most out of it. Here, decision trees in the form of random forests will be used since they can readily handle categorical data in a multiclass classifcation problem. </p>

<p>The process is divided into two steps:</p>

<ol>
<li>A feature extraction algorithm processes all the images and computes a number of observables in order to reduce the complexity of the problem.</li>
<li>Using those observables, the random forest classifer is trained and then used to classify the images according to the digits they represent. </li>
</ol><p>The functionality of the random forest classifier is enhanced by providing <em>scores</em> (or probability estimates) of each digit for every image. Those scores are then calibrated to the true classifier response. Instead of being able to only predict the most likely digit an image represents (e.g.: this image represents a 3), the calibration procedure provides us with the probabilities for an image to represent a given digit (e.g.: 10% chance it's a 0, 1% chance it's a 1, 5% chance it's a 2, 65% chance it's a 3, etc. ).</p>

<h2>
<a name="installation" class="anchor" href="#installation"><span class="octicon octicon-link"></span></a>Installation</h2>

<p>A makefile is provided for unix-like operating systems (tested on Mac OS X 10.7.5). Requirements: </p>

<ul>
<li>C++11 compliant c++ compiler (e.g. g++ 4.8)</li>
<li>
<a href="http://www.boost.org/">boost</a> library</li>
<li>Python 2.x (e.g. version 2.7.1)</li>
<li>
<a href="http://swig.org">swig</a> 2.x </li>
<li>
<a href="http://root.cern.ch">ROOT</a> 5.x </li>
<li>
<a href="http://opencv.org/">OpenCV</a> 2.4.6.1</li>
</ul><p>(All the above can be obtained through <a href="http://www.macports.org/">macports</a> for Max OS X systems.)</p>

<ul>
<li>
<code>simplefwk-services</code> <a href="http://chapleau.github.io/simplefwk-services">package</a></li>
<li>
<code>simplefwk-utilitytoolsinterfaces</code> <a href="http://chapleau.github.io/utilitytoolsinterfaces">package</a></li>
<li>
<code>simplefwk-utilitytools</code> <a href="http://chapleau.github.io/utilitytools">package</a></li>
</ul><h2>
<a name="usage" class="anchor" href="#usage"><span class="octicon octicon-link"></span></a>Usage</h2>

<p>The input dataset can be obtained from the <a href="http://www.kaggle.com/c/digit-recognizer/data">kaggle</a> website. It consists in 42000 28-by-28 pixels gray-scale images of handwritten digits. Eventhough the modules and algorithms are implemented in C++, most of the steering is done from Python (thanks to swig's automatic interface generation). </p>

<h3>
<a name="feature-extraction" class="anchor" href="#feature-extraction"><span class="octicon octicon-link"></span></a>Feature Extraction</h3>

<p>Ideally, one would use the 784 pixel (integer) values of each image as features, leaving to the classifier the task of learning about correlations and building an optimal model of our data in order to yield reliable predictions. A maximal amount of information can then be exploited. An obvious drawback is a lengthy training process. In order to speed up the training of the classifier, a set of 70 potentially discriminating features are systematically computed from each image. </p>

<p>As a first step, the image is preprocessed:</p>

<ul>
<li>A <a href="http://en.wikipedia.org/wiki/Gaussian_blur">Gaussian blur</a> is applied to smooth out the image (see <code>src/GaussianBlurTool.cxx</code>).<br>
</li>
<li>A <a href="http://en.wikipedia.org/wiki/Canny_edge_detector">Canny edge detection</a> algorithm (see <code>src/CannyEdgeTool.cxx</code>) is used to convert the original image to a binary image consisting of only the edges.
<br><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/6_plain.png" alt="6 plain" height="300" width="300"><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/6_processed.png" alt="6 plain" height="300" width="300">
</li>
</ul><p>The two images above represent the same instance of a <em>6</em>, before (left) and after (right) the preprocessing steps (the extra lines, markers, colors, are just visual aids). The features are computed from the preprocessed images using the pixel positions and includes (see <code>src/FeX.cxx</code>):</p>

<ul>
<li>The center of mass (depicted by the black pixel in the image here)</li>
<li>Axis minimizing the second (inertia) moment (blue axis, red axis is orthogonal)</li>
<li>Measure of symmetry of the image with respect to the axes defined by the second moments.</li>
<li>Five number summary (minimum, first quartile, median, third quartile, maximum) of the pixel positions and angle with respect to the inertia axes, computed in each quadrant (the markers identify the different quadrants).</li>
</ul><p>All the features are saved as a TTree in a ROOT file (see <code>simplefwk-utilitytools</code> package for more details) for easy and efficient future access. The <code>run/run.py</code> Python file can be used to run the feature extraction algorithm (from the <code>run/</code> directory, using the Bash shell):</p>

<pre lang="shell"><code>PYTHONPATH=$PYTHONPATH:`pwd`/../../ python ./run.py
</code></pre>

<h3>
<a name="classification-by-random-forests" class="anchor" href="#classification-by-random-forests"><span class="octicon octicon-link"></span></a>Classification by Random Forests</h3>

<p>The classification of the dataset is done using <a href="http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm">random forests</a> using the <a href="http://opencv.org/">OpenCV</a> machine learning library. It involves three distinct steps: the cross-validation, training, and testing phases. The whole process can be run using the provided Python steering file (from the <code>run/</code> directory, using the Bash shell):</p>

<pre lang="shell"><code>PYTHONPATH=$PYTHONPATH:`pwd`/../../ python ./run_cl.py
</code></pre>

<h4>
<a name="probability-estimation-trees" class="anchor" href="#probability-estimation-trees"><span class="octicon octicon-link"></span></a>Probability estimation trees</h4>

<p>In many implementations of the random forests classifier (such as the one present in the OpenCV library), the <em>forest</em> consists in an ensemble of classification trees. Each leaf in such trees is associated with a single class label that is determined by a vote during the training phase. Modifications were made to the OpenCV impementation (see <code>src/rtrees.hpp</code>) in order to operate on an ensemble of probability estimation trees instead. In this case, the relative class frequency in a leaf that is obtained during the training phase is used as an estimation of the class membership probability. The estimated class probablity for the forest is taken as the average, over the whole ensemble, of the single tree relative class frequency (see <a href="http://people.dsv.su.se/%7Ehenke/papers/bostrom07c.pdf">this paper</a>).</p>

<p>Those probability estimates will be denoted as <em>scores</em>. If two events are classified as members of a class <em>c</em>, the one with the highest score is more likely to be a true member of class <em>c</em>. In order to be able to interpret those scores as the chance of membership of a class, calibration is necessary. The calibration maps scores to empirical class membership probabilities. Accurate class probability estimates are necessary when combining the output of the classifier with other independent sources of information or with different classifiers. (see <a href="http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf">this paper</a>). </p>

<p>The calibration is discussed in more details in the cross-validation section.</p>

<h4>
<a name="cross-validation--training" class="anchor" href="#cross-validation--training"><span class="octicon octicon-link"></span></a>Cross-validation &amp; training</h4>

<p>A <em>k</em>-fold cross-validation (with <em>k</em> set to 4 for practical purposes) is performed on half of the dataset. It consists in randomly partitioning the sample into <em>k</em> subsamples of roughly equal size, leaving out one for validation/testing purposes and using the remaining subsamples as training data. The procedure is repeated <em>k</em> times, each time using a different subsample for testing, and the results are combined. Each event can then be tested using a classifier trained on an independent set of events. </p>

<p>Each instance of the cross-validation procedure is executed in parallel in a unique thread. The original OpenCV implementation was modified in order to (notably) monitor the progress of the computations:
<br><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/cv_terminal.png" alt="cv"><br></p>

<p>The main purpose of cross-validation is to find out the best classifier parameter values to be used. This can be done by scanning the parameter space for the set of values that allows the classifier to perform at its best. A good set of parameter values were found that way and are used in <code>run/run_cl.py</code>. </p>

<p>The calibration functions (i.e. mappings from scores to probability estimates) is determined during cross-validation. Ideally, when dealing with a multiclass classifier, a non-trivial multidimensional mapping function would need to be determined. Here, for simplicity, a calibration is computed for each class individually. The calibrated probability estimates are then normalized so that they sum to 1. Binned reliability graphs are used to compute single-class calibration mappings :
<br><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/reliability_graph_8.png" alt="cv" height="350" width="465"><br></p>

<p>Here, the markers represent the fraction of predictions (for the digit <em>8</em> in this case) with a given raw score which are correct. The overlayed colored curves are fitted sigmoid functions. The best fitted curve (i.e. the one with the lowest Chi2/NDF) is taking as the calibration function. It is to be noted that the bin size is kept constant for all classes (digits). This choice is technically not optimal because statistical fluctuations can be important for some classes (that is when the scores are not distributed very evenly) which can affect the empirical class probability estimates.</p>

<p>The single-class calibration functions are saved for the testing phase where they will be used to calibrate raw scores. As a last step, the classifier is trained again using the full training data (i.e. half the dataset).</p>

<h4>
<a name="testing" class="anchor" href="#testing"><span class="octicon octicon-link"></span></a>Testing</h4>

<p>The classifier is tested on an independent set of events (the other half of the dataset). When classifying an event, raw scores are obtained directly from the random forest for each class (i.e. digit). The scores are then calibrated using the mappings derived during the cross-validation procedure. In order to be able to interpret those as probabilities, they need to sum to unity. Instead of applying a constant normalization factor, the mapping functions are allowed to simultaneously <em>float</em> in such a way that the proper normalization is achieved. Each calibration function is thus allowed to shift from its nominal position by a multiple of the one-sigma uncertainty (estimated by a 68% confidence interval, beware of its tricky interpretation!) on the fit results, at a given score value. The smallest possible shifts that give rise to the desired normalization are chosen by minimizing a Gaussian cost function. The largest shifts will therefore be applied to  mappings with large (fit) uncertainties.</p>

<p>To test the calibration, reliability graphs are produced (here for digit <em>8</em> on the left, and digit <em>6</em> on the right):
<br><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/reliability_graph_test_8.png" alt="cv" height="260" width="325"><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/reliability_graph_test_6.png" alt="cv" height="260" width="325"><br></p>

<p>The error bars represent the statistical uncertainty added in quadrature with the propagated fit uncertainties. As it can be seen from the two graphs above, the calibration seems to work very well for some classes but less for others. This might be an indication that our simplistic approach to this complex multi-dimensional problem is not quite optimal.</p>

<p>All results are written out a TTree which makes the post-processing of the classifier output easy. The Python file <code>run/run_post_analysis.py</code> contains a simple analysis of the results. For instance, the success rate (how many digits were correcly identified) is just above 90%. This is rather low as expected (see the introduction). With the estimated class probabilities, one can make more sophisticated analysis. For example, one can ask how many times the correct digit is predicted by the most probable <strong>or</strong> the second to most probable one. In the present case, this amounts to approximatley 96%. If purity is a concern, one can select a subset of the classified events which exhibit high class membership probabilities: by only considering events with predicted class probability &gt; 90%, one gets a 99% success rate. This comes, however, to the expense of a low acceptance rate. The following graph illustrates the relation between the achievable success rates and the corresponding acceptances for this dataset.
<br><img src="https://raw.github.com/chapleau/DigitRecog/master/doc/accep_vs_accuracy.png" alt="cv" height="350" width="465"><br></p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/chapleau/DigitRecog/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/chapleau/DigitRecog/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/chapleau/DigitRecog"></a> is maintained by <a href="https://github.com/chapleau">chapleau</a>.</p>

          <p><h1>see also: <a href="http://chapleau.github.io/simplefwk-services"><b>Simple Framework</b></a> </h1> </p> 

          <p>This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the Architect theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.</p>
        </aside>
      </div>
    </div>

  
  </body>
</html>
